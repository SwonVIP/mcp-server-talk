Summary of Knowledge About MCP Servers
MCP servers are lightweight programs, each designed to expose specific capabilities through the standardized Model Context Protocol (MCP). They act as a crucial communication bridge between AI language models (LLMs) and external tools or services. The core purpose of MCP is to allow LLMs to interact seamlessly with a variety of systems, thereby enhancing agent capabilities while maintaining secure boundaries for data access and action execution. MCP provides a standardised way for applications to share contextual information, expose tools and capabilities to AI systems, and build composable integrations and workflows. The protocol is often described using the analogy of a "USB-C port for AI applications", standardising how AI models connect to different data sources and tools, much like USB-C standardises device connections to peripherals. This transforms the challenge of building M integrations for N systems from an M x N problem into an M + N problem, where developers build N MCP servers and M MCP clients.
At its foundation, MCP follows a client-server architecture. Key components interacting with MCP servers include:
•
MCP Hosts: These are the applications the user interacts with, such as Claude Desktop, IDEs (like Cursor or Winsoft), or custom AI agents. A host application can connect to multiple servers.
•
MCP Clients: These reside within the Host application and maintain a 1:1 connection with a specific MCP server, managing the communication.
•
MCP Servers: These are the external programs providing the capabilities. They can securely access Local Data Sources (like files, databases, and services on a user's machine) or connect to Remote Services (external systems via APIs).
Communication within MCP uses JSON-RPC 2.0 messages. The communication between the AI model and the MCP server is facilitated by a Transport Layer, which supports mechanisms like standard input/output (stdio) for local connections or HTTP via Server-Sent Events (SSE) for multi-client or remote applications.
MCP Servers primarily expose three types of features to clients:
•
Tools: These are functions that the LLM can call to perform specific actions. Tools are Model-controlled, meaning the LLM decides when to invoke them, typically based on user queries or required actions. They represent arbitrary code execution and can wrap external API calls or database queries. Examples include searching Jira or Confluence. Server SDKs often use decorators to map functions to LLM tool requests.
•
Resources: These provide context and data, which can be used by the user or the AI model. Resources are structured data streams, like files, logs, or database content. Unlike Tools, accessing Resources is typically the task of the Host application, not solely decided by the model, and they are conceptually similar to GET requests without computation or side effects, returning predictable results. An IDE integration might use a Resource to provide file context to the model.
•
Prompts: These are templated messages and workflows for users, designed to help the LLM efficiently interact with Tools or Resources. Prompts are User-controlled, not decided by the LLM. They ensure requests are properly structured and consistent, which is useful for complex systems. Prompts can guide Tool use (e.g., specifying required API parameters), frame Resource queries (e.g., defining log format/filters), optimise API requests (e.g., providing templates), and manage response formatting (e.g., guiding the LLM on interpreting external responses). The MCP server developer is considered the domain expert who provides these useful Prompt templates.
Clients MAY offer one feature to servers: Sampling, which allows for server-initiated agentic behaviour and recursive LLM interactions. However, users MUST explicitly approve any LLM sampling requests and retain control over whether it happens, the prompt sent, and what results the server sees.
The typical communication flow involves the LLM selecting an appropriate Tool or Resource based on a user request. This request is sent as a JSON-RPC message to the MCP server. The server uses a Schema Validator to ensure the request is valid, translates it into the necessary external API call, processes and formats the external response, and returns it to the LLM for integration into the final user output. Pydantic models can be used for request validation and response formatting.
SDKs are available in numerous languages (Python, TypeScript, Java, Kotlin, C#, Swift, plus community Go and Rust) to facilitate both client and server implementation. Libraries like Fast MCP for Python can simplify server development using decorators. Servers can be deployed as Docker Containers (using environment variables), as a Local Python Installation, as SSE Servers for multi-client use, or as Remote Servers which often require complex authentication like OAuth. There is no single public registry for MCP servers yet, though community efforts exist, and Anthropic has announced they are working on one. Due to security implications, users SHOULD review the code of servers they use. Testing servers can be complex, requiring external service integration and credential management. Tools like the MCP Inspector aid in debugging.
Security Concerns and Attack Vectors
Given that MCP servers often interact with sensitive systems and APIs and enable arbitrary code execution via Tools, maintaining strong security practices is essential. MCP adoption introduces new risks related to misalignment between agent behaviour and user expectations, uncontrolled execution, and presents a novel attack surface, including new software supply chain threats.
A critical principle is User Consent and Control: Users MUST explicitly consent to and understand all data access and operations, and MUST retain control over what data is shared and actions taken. Hosts MUST obtain explicit user consent before exposing user data to servers or invoking Tools, and MUST NOT transmit resource data elsewhere without consent. However, current implementations often lack clear or consistent user approval flows. Some may ask for permission once and apply that consent to subsequent, potentially dangerous uses without re-prompting, creating a significant risk if an attacker sends a benign request followed by a malicious one that exploits the pre-granted permission.
Tools are a major security concern as they represent arbitrary code execution. Descriptions of tool behaviour (like annotations) from untrusted servers SHOULD be considered untrusted. Resources need robust access controls and validation to prevent data breaches or poisoning. Prompts need protection against injection attacks that could redirect AI outputs.
Specific attack vectors and security concerns include:
•
Using Unvetted Servers: MCP servers are often pulled directly from the internet and executed locally with sensitive credentials, which is inherently risky. Integrating with unverified servers without thorough scanning and assessment is dangerous, akin to installing untrusted software packages.
•
Limited Invocation Controls: Most server implementations default to broad access for the LLM, potentially leading to oversharing and excessive permissions, as seen with an unofficial Salesforce MCP lacking authentication.
•
Lack of Observability/Approval Workflows: The absence of built-in monitoring makes tracing activity and incident response harder. MCP currently lacks out-of-the-box human-in-the-loop workflows for critical actions, meaning users or security teams cannot easily review and approve high-risk function calls before execution.
•
Combinations of MCP Servers: Attackers can exploit sequences of actions involving tools from multiple servers. By injecting commands into data the client consumes (an indirect prompt injection), an attacker can trigger tool calls that leverage permissions previously granted for benign purposes, leading to actions like data exfiltration without requiring new user prompts or code execution on the system. This makes complex threats difficult to reason about.
•
Tool Name TypoSquatting: MCP allows multiple servers to define tools with the same name, and there's no inherent way to distinguish them by name alone. When tools are loaded, names can overwrite each other, and the model might only be able to call the latest one. Malicious servers can exploit this by defining tools with names identical to trusted ones (e.g., a malicious push_files tool shadowing a legitimate one), leading the model to silently execute the malicious version. This risk increases with remote servers and can occur without restarting the LLMs.
•
"Inadvertent Double Agents": Many third-party local servers either by design or inadvertently allow arbitrary code execution, meaning they can be controlled by indirect prompt injections originating from data consumed by the client, not just direct user commands.
•
Unauthenticated Local Servers: Local SSE-based MCP servers typically bind to a port on localhost and are accessible to other processes on the machine. The transport layers do not enforce authentication by default; this is left to the server developer, and nearly all servers currently do not enforce authentication.
•
Chrome Extension Sandbox Escape: A severe vulnerability where a Chrome extension, without special permissions, can connect to a local, unauthenticated SSE MCP server running on localhost, bypassing Chrome's sandbox and allowing unrestricted access to MCP tools. This can lead to unauthenticated filesystem access or even full machine takeover if the server exposes such capabilities. This is a real and underestimated attack surface.
•
Confused Deputy Problem: Attackers can exploit MCP proxy servers using static client IDs with third-party authorization servers that lack dynamic registration. By crafting a malicious link, they can potentially obtain access tokens for the MCP server via the user's existing consent cookie for the static ID, bypassing explicit user approval for the malicious request. Mitigation requires the proxy server to obtain user consent for each dynamically registered client.
•
Token Passthrough (Anti-pattern): An explicitly forbidden practice where an MCP server accepts tokens from a client and passes them to a downstream API without validating they were issued specifically for the MCP server. This bypasses downstream security controls, creates audit trail issues, allows data exfiltration via a compromised token, and breaks trust boundaries. MCP servers MUST NOT accept such tokens.
Mitigation strategies and best practices involve applying traditional security measures (supply chain security, code scanning, version pinning), building robust consent and authorization flows, implementing access controls, providing clear documentation, and considering privacy. Authentication SHOULD be enforced, particularly for non-stdio implementations, and MCP needs a clear identity model. Runtime monitoring and tracing are critical for visibility. Human-in-the-loop approvals are necessary for critical actions. Users SHOULD use only verified or vendor-approved MCP servers and review server code. Containerisation is recommended as it provides isolation, portability, and verification, mitigating risks of running untrusted servers directly. Container-based solutions can offer secure secrets handling and can be used to build a centralised security checkpoint (a Gateway or proxy) to detect threats like Rug Pull, Shadowing, and Tool Poisoning before they reach the client. The specification is evolving to include features like tool annotations (readOnlyHint, destructiveHint) to help define and enforce tool boundaries, which can be leveraged by secure runtimes. Developers SHOULD follow API security best practices.
